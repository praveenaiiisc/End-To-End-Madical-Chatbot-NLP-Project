{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.1.2 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.1.2\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting transformers==4.38.2\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "Collecting datasets==2.16.1\n",
      "  Using cached datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting accelerate==0.26.1\n",
      "  Using cached accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting evaluate==0.4.1\n",
      "  Using cached evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting bitsandbytes==0.42.0\n",
      "  Using cached bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting trl==0.7.11\n",
      "  Using cached trl-0.7.11-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting peft==0.8.2\n",
      "  Using cached peft-0.8.2-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langchain in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (0.2.6)\n",
      "Requirement already satisfied: sentence-transformers in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (2.2.2)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.8.0.post1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: filelock in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from transformers==4.38.2) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from transformers==4.38.2) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from transformers==4.38.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from transformers==4.38.2) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from transformers==4.38.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from transformers==4.38.2) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from transformers==4.38.2) (2.32.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n",
      "  Using cached tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from transformers==4.38.2) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from transformers==4.38.2) (4.66.4)\n",
      "Collecting pyarrow>=8.0.0 (from datasets==2.16.1)\n",
      "  Using cached pyarrow-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets==2.16.1)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.16.1)\n",
      "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pandas (from datasets==2.16.1)\n",
      "  Using cached pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting xxhash (from datasets==2.16.1)\n",
      "  Using cached xxhash-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.16.1)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1)\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from datasets==2.16.1) (3.9.5)\n",
      "Requirement already satisfied: psutil in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from accelerate==0.26.1) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from accelerate==0.26.1) (2.3.1)\n",
      "Collecting responses<0.19 (from evaluate==0.4.1)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: scipy in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from bitsandbytes==0.42.0) (1.14.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.7.11)\n",
      "  Using cached tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from langchain) (0.2.11)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from langchain) (0.1.83)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from langchain) (8.4.2)\n",
      "Requirement already satisfied: scikit-learn in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: Pillow in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from requests->transformers==4.38.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from requests->transformers==4.38.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from requests->transformers==4.38.2) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from requests->transformers==4.38.2) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: sympy in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.1) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.1) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.26.1) (12.5.82)\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl==0.7.11)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl==0.7.11)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.7.11)\n",
      "  Using cached shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.16.1)\n",
      "  Using cached multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==2.16.1)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==2.16.1)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain) (3.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from sympy->torch>=1.10.0->accelerate==0.26.1) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.11)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "Using cached datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "Using cached accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "Using cached evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Using cached bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "Using cached trl-0.7.11-py3-none-any.whl (155 kB)\n",
      "Using cached peft-0.8.2-py3-none-any.whl (183 kB)\n",
      "Using cached sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "Using cached faiss_cpu-1.8.0.post1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Using cached pyarrow-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Using cached tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached tyro-0.8.5-py3-none-any.whl (103 kB)\n",
      "Using cached multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "Using cached pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, shtab, pyarrow-hotfix, pyarrow, mdurl, fsspec, faiss-cpu, docstring-parser, dill, responses, pandas, multiprocess, markdown-it-py, bitsandbytes, tokenizers, rich, tyro, transformers, datasets, accelerate, trl, sentence-transformers, peft, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.42.3\n",
      "    Uninstalling transformers-4.42.3:\n",
      "      Successfully uninstalled transformers-4.42.3\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 2.2.2\n",
      "    Uninstalling sentence-transformers-2.2.2:\n",
      "      Successfully uninstalled sentence-transformers-2.2.2\n",
      "Successfully installed accelerate-0.26.1 bitsandbytes-0.42.0 datasets-2.16.1 dill-0.3.7 docstring-parser-0.16 evaluate-0.4.1 faiss-cpu-1.8.0.post1 fsspec-2023.10.0 markdown-it-py-3.0.0 mdurl-0.1.2 multiprocess-0.70.15 pandas-2.2.2 peft-0.8.2 pyarrow-16.1.0 pyarrow-hotfix-0.6 pytz-2024.1 responses-0.18.0 rich-13.7.1 sentence-transformers-3.0.1 shtab-1.7.1 tokenizers-0.15.2 transformers-4.38.2 trl-0.7.11 tyro-0.8.5 tzdata-2024.1 xxhash-3.4.1\n",
      "Collecting unstructured\n",
      "  Using cached unstructured-0.14.9-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting chardet (from unstructured)\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filetype (from unstructured)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Using cached python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lxml (from unstructured)\n",
      "  Using cached lxml-5.2.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: nltk in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from unstructured) (3.8.1)\n",
      "Collecting tabulate (from unstructured)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: requests in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from unstructured) (2.32.3)\n",
      "Collecting beautifulsoup4 (from unstructured)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting emoji (from unstructured)\n",
      "  Using cached emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: dataclasses-json in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from unstructured) (0.5.14)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Using cached python_iso639-2024.4.27-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: numpy<2 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from unstructured) (1.26.4)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Using cached rapidfuzz-3.9.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting backoff (from unstructured)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from unstructured) (4.12.2)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Using cached unstructured_client-0.23.8-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting wrapt (from unstructured)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from unstructured) (4.66.4)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->unstructured)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from dataclasses-json->unstructured) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from langdetect->unstructured) (1.16.0)\n",
      "Requirement already satisfied: click in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from nltk->unstructured) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from nltk->unstructured) (2024.5.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from requests->unstructured) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from requests->unstructured) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from requests->unstructured) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from requests->unstructured) (2024.7.4)\n",
      "Collecting dataclasses-json (from unstructured)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deepdiff>=6.0 (from unstructured-client->unstructured)\n",
      "  Using cached deepdiff-7.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx>=0.27.0 (from unstructured-client->unstructured)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured)\n",
      "  Using cached jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: packaging>=23.1 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from unstructured-client->unstructured) (24.1)\n",
      "Requirement already satisfied: pypdf>=4.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from unstructured-client->unstructured) (4.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from unstructured-client->unstructured) (2.9.0)\n",
      "Collecting requests-toolbelt>=1.0.0 (from unstructured-client->unstructured)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting ordered-set<4.2.0,>=4.1.0 (from deepdiff>=6.0->unstructured-client->unstructured)\n",
      "  Using cached ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting anyio (from httpx>=0.27.0->unstructured-client->unstructured)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.0->unstructured-client->unstructured)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sniffio (from httpx>=0.27.0->unstructured-client->unstructured)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Using cached unstructured-0.14.9-py3-none-any.whl (2.1 MB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Using cached emoji-2.12.1-py3-none-any.whl (431 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached lxml-5.2.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "Using cached python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
      "Using cached python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Using cached rapidfuzz-3.9.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached unstructured_client-0.23.8-py3-none-any.whl (40 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
      "Using cached deepdiff-7.0.1-py3-none-any.whl (80 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Using cached ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: filetype, wrapt, tabulate, soupsieve, sniffio, rapidfuzz, python-magic, python-iso639, ordered-set, lxml, langdetect, jsonpath-python, h11, emoji, chardet, backoff, requests-toolbelt, httpcore, deepdiff, dataclasses-json, beautifulsoup4, anyio, httpx, unstructured-client, unstructured\n",
      "  Attempting uninstall: dataclasses-json\n",
      "    Found existing installation: dataclasses-json 0.5.14\n",
      "    Uninstalling dataclasses-json-0.5.14:\n",
      "      Successfully uninstalled dataclasses-json-0.5.14\n",
      "Successfully installed anyio-4.4.0 backoff-2.2.1 beautifulsoup4-4.12.3 chardet-5.2.0 dataclasses-json-0.6.7 deepdiff-7.0.1 emoji-2.12.1 filetype-1.2.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpath-python-1.0.6 langdetect-1.0.9 lxml-5.2.2 ordered-set-4.1.0 python-iso639-2024.4.27 python-magic-0.4.27 rapidfuzz-3.9.4 requests-toolbelt-1.0.0 sniffio-1.3.1 soupsieve-2.5 tabulate-0.9.0 unstructured-0.14.9 unstructured-client-0.23.8 wrapt-1.16.0\n",
      "Collecting pdfminer\n",
      "  Using cached pdfminer-20191125-py3-none-any.whl\n",
      "Collecting pycryptodome (from pdfminer)\n",
      "  Using cached pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Using cached pycryptodome-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Installing collected packages: pycryptodome, pdfminer\n",
      "Successfully installed pdfminer-20191125 pycryptodome-3.20.0\n",
      "Collecting pdfminer.six\n",
      "  Using cached pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages (from pdfminer.six) (3.3.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six)\n",
      "  Using cached cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six)\n",
      "  Using cached cffi-1.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "Using cached cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
      "Using cached cffi-1.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (477 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pycparser, cffi, cryptography, pdfminer.six\n",
      "Successfully installed cffi-1.16.0 cryptography-42.0.8 pdfminer.six-20231228 pycparser-2.22\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch & other libraries\n",
    "!pip install \"torch==2.1.2\" tensorboard\n",
    "\n",
    "# Install Hugging Face libraries\n",
    "!pip install  --upgrade \\\n",
    "  \"transformers==4.38.2\" \\\n",
    "  \"datasets==2.16.1\" \\\n",
    "  \"accelerate==0.26.1\" \\\n",
    "  \"evaluate==0.4.1\" \\\n",
    "  \"bitsandbytes==0.42.0\" \\\n",
    "  \"trl==0.7.11\" \\\n",
    "  \"peft==0.8.2\" \\\n",
    "    \"langchain\" \\\n",
    "\"sentence-transformers\" \\\n",
    "\"faiss-cpu\"\n",
    "!pip install unstructured\n",
    "!pip install pdfminer\n",
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "# from langchain.chains import RetrievalQA  # for question Answering \n",
    "# from langchain.embeddings import HuggingFaceEmbeddings  # import hagingface embedding\n",
    "from langchain.vectorstores import Pinecone   # for data base\n",
    "import pinecone\n",
    "# from langchain.document_loaders import PyPDFLoader, DirectoryLoader  # for pdf and directory dataset\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain.llms import CTransformers # neeed bcz i use quantize model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IPython.display import display_markdown\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import pipeline\n",
    "import transformers\n",
    "from langchain.document_loaders import UnstructuredPDFLoader,PDFMinerLoader,TextLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    " \n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    token='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',\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.float32,\n",
    "        # \"quantization_config\": {\"load_in_4bit\": True},\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "terminators =  [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pdf file Path for RAG\n",
    "pdf_file_path = \"/home/praveent/End-To-End-Madical-Chatbot-NLP-Project-1/data/Medical_book.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3_8B_gen:\n",
    "    def __init__(self,pipeline):\n",
    "        self.pipeline= pipeline\n",
    "        \n",
    "    @staticmethod\n",
    "    def generate_prompt(query,retrieved_text):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Answer the Question for the Given below context and information and not prior knowledge, only give the output result \\n\\ncontext:\\n\\n{}\".format(retrieved_text) },\n",
    "            {\"role\": \"user\", \"content\": query},]\n",
    "        return pipeline.tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True)\n",
    "    \n",
    "    def generate(self,query,retrieved_context):\n",
    "        prompt = self.generate_prompt(query ,retrieved_context)\n",
    "        output =  pipeline(prompt,max_new_tokens=512,eos_token_id=terminators,do_sample=True,\n",
    "                            temperature=0.7,top_p=0.9,)         \n",
    "        return output[0][\"generated_text\"][len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Langchain_RAG:\n",
    "    def __init__(self,pdf_file_path):\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "        self.pdf_file_path = pdf_file_path\n",
    "        print(\"loading pdf file , this may take time to process\")\n",
    "        self.loader = loader = PDFMinerLoader(self.pdf_file_path)   \n",
    "        self.data = self.loader.load()\n",
    "        print(\"<< pdf file loaded\")\n",
    "        print(\"<< chunking\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"])\n",
    "        self.texts = text_splitter.split_documents(self.data)\n",
    "        print(\"<< chunked\")\n",
    "        self.get_vec_value= FAISS.from_documents(self.texts, self.embeddings)\n",
    "        print(\"<< saved\")\n",
    "        self.retriever = self.get_vec_value.as_retriever(search_kwargs={\"k\": 4})\n",
    "    def __call__(self,query):\n",
    "        rev = self.retriever.get_relevant_documents(query) \n",
    "        return \"\".join([i.page_content for i in rev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praveent/End-To-End-Madical-Chatbot-NLP-Project-2/.conda/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pdf file , this may take time to process\n",
      "<< pdf file loaded\n",
      "<< chunking\n",
      "<< chunked\n",
      "<< saved\n"
     ]
    }
   ],
   "source": [
    "text_gen = Llama3_8B_gen(pipeline=pipeline)\n",
    "retriever = Langchain_RAG(pdf_file_path=pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "According to the provided context, an allergy is a type of immune reaction where the immune system responds to harmless, everyday substances such as pollen, dust, and animal danders by producing specific proteins called antibodies. These antibodies bind to the offending substance (an allergen) and trigger a series of chemical reactions designed to protect the body from infection, but in this case, the reaction is unnecessary and can cause harm."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def Rag_qa(query):\n",
    "    retriever_context = retriever(query)\n",
    "    resut = text_gen.generate(query,retriever_context)\n",
    "    return resut\n",
    "\n",
    "display_markdown(Rag_qa(\"What are Allergies\"),raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input_query :  What is Acne?\n",
      "Chatbot_Response :  According to the given context, Acne is a disease that has a characteristic appearance and is not difficult to diagnose. It is a skin condition that occurs when the pores on the skin become clogged with dead skin cells, oil, and bacteria, leading to the formation of pimples, blackheads, and whiteheads. The condition can appear on various parts of the body, including the face, chest, shoulders, and back.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input_query :  Who is Praveen?\n",
      "Chatbot_Response :  There is no mention of a person named Praveen in the given context. The text only discusses Ayurvedic principles, doshas, and the concept of prakriti, but does not mention a specific individual named Praveen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input_query :  How we can save the life of a person, who has fever\n",
      "Chatbot_Response :  According to the provided context, for a person with balantidiasis, the following steps can be taken to save their life:\n",
      "\n",
      "1. Bed rest may be recommended along with medications to reduce fever and/or pain.\n",
      "2. Medications may be prescribed to relieve symptoms of the illness.\n",
      "3. Topical corticosteroids may be used to relieve symptoms and shorten the course of the disease.\n",
      "4. Hospitalization may be required for severe pneumonia in infants and for EKC (to prevent blindness).\n",
      "\n",
      "For a person with adenovirus infection, the following steps can be taken:\n",
      "\n",
      "1. Practicing good personal hygiene and avoiding people with infectious illnesses can reduce the risk of developing adenovirus infection.\n",
      "2. Proper handwashing can prevent the spread of the virus by oral-fecal transmission.\n",
      "3. Sterilization of instruments and solutions used in the eye can prevent the spread of EKC, as can adequate chlorination of swimming pools.\n",
      "\n",
      "For a person with ARDS (Acute Respiratory Distress Syndrome), the following steps can be taken:\n",
      "\n",
      "1. Providing enough fluid, by vein if necessary, to prevent dehydration.\n",
      "2. Maintaining the patient's nutritional state, again by vein, if oral intake is not sufficient.\n",
      "3. Prompt treatment is necessary to prevent bacterial pneumonia, which can develop at some point.\n",
      "4. Modern treatment can help about half of all patients with ARDS survive.\n",
      "5. Those who do live usually recover completely, with little or no long-term breathing difficulty. Lung scarring is a risk after a long period on a ventilator, but it may improve in the months after the patient is taken off ventilation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input_query :  \n",
      "Chatbot_Response :  There is no question provided. Please provide the question you would like me to answer based on the given context and information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input_query :  \n",
      "Chatbot_Response :  Based on the given context and information, I will answer the question:\n",
      "\n",
      "What is the treatment for brain tumors?\n",
      "\n",
      "According to the text, brain tumors are treated by multidisciplinary teams of highly skilled specialists whose decisions are based on:\n",
      "\n",
      "* Results of diagnostic tests\n",
      "* Tumor size, position, and growth pattern\n",
      "* The patient's health history and current medical status\n",
      "* The wishes of the patient and his family\n",
      "\n",
      "The treatment options mentioned in the text are:\n",
      "\n",
      "* Surgery: for accessible brain tumors that can be removed without problems\n",
      "* Diagnostic tests: including myelography, lumbar puncture, digital holography, computed tomography (CT) scan, magnetic resonance imaging (MRI), and cerebral angiography\n",
      "* Biopsy: to examine the tumor tissue and identify the kind of cells it contains\n",
      "* Fine needle aspiration biopsy: to withdraw fluid from lymph nodes located near the growth to make sure the cancer has not spread to these nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input_query :  \n",
      "Chatbot_Response :  The given text appears to be a part of the \"Gale Encyclopedia of Medicine 2\", specifically the entry on \"Brain Tumor\".\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     user_input_query\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInput Prompt:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input_query : \u001b[39m\u001b[38;5;124m\"\u001b[39m, user_input_query)\n\u001b[1;32m      4\u001b[0m     result\u001b[38;5;241m=\u001b[39mRag_qa(user_input_query)\n",
      "File \u001b[0;32m~/End-To-End-Madical-Chatbot-NLP-Project-1/.conda/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/End-To-End-Madical-Chatbot-NLP-Project-1/.conda/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input_query=input(f\"Input Prompt:\")\n",
    "    print(\"user_input_query : \", user_input_query)\n",
    "    result=Rag_qa(user_input_query)\n",
    "    print(\"Chatbot_Response : \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -qU \\\n",
    "#   pinecone-client==3.1.0 \\\n",
    "#   pinecone-datasets==0.7.0 \\\n",
    "#   sentence-transformers==2.2.2 \\\n",
    "#   pinecone-notebooks==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = \"189745e7-5942-433c-9f81-5076e264a0ff\"  # generate api key from pinecode website\n",
    "# PINECONE_API_ENV = \"aped-4627-b74a\"   # to create env for sentence embedding model from haggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "import os\n",
    "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "index_name = 'medical-chatbot'\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=384,  # dimensionality of minilm\n",
    "        metric=\"cosine\",\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'text_chunks' is a list containing text data and 'embeddings' is a function that generates embeddings\n",
    "text_data = [t.page_content for t in c]\n",
    "embeddings_data = [embeddings.embed_query(test) for test in text_data]\n",
    "embeddings_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 11904}},\n",
      " 'total_vector_count': 11904}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "import numpy as np\n",
    "\n",
    "shape = np.array(embeddings_data).shape\n",
    "ids = [str(i) for i in range(shape[0])]\n",
    "# create list of metadata dictionaries\n",
    "meta = [{'text': t.page_content}  for t in text_chunks]\n",
    "\n",
    "# create list of (id, vector, metadata) tuples to be upserted\n",
    "to_upsert = list(zip(ids, embeddings_data , meta))\n",
    "\n",
    "for i in range(0, shape[0], batch_size):\n",
    "    i_end = min(i+batch_size, shape[0])\n",
    "    index.upsert(vectors=to_upsert[i:i_end])\n",
    "\n",
    "# let's view the index statistics\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Initializing the Pinecone\n",
    "# pinecone.init(api_key=PINECONE_API_KEY,\n",
    "#               environment=PINECONE_API_ENV)\n",
    "\n",
    "# index_name=\"medical-chatbot\"\n",
    "\n",
    "# #Creating Embeddings for Each of The Text Chunks & storing\n",
    "# docsearch=Pinecone.from_texts([t.page_content for t in text_chunks], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-pinecone\n",
      "  Using cached langchain_pinecone-0.1.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting langchain-core<0.3,>=0.1.52 (from langchain-pinecone)\n",
      "  Using cached langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langchain-pinecone) (1.26.4)\n",
      "Collecting pinecone-client<4.0.0,>=3.2.2 (from langchain-pinecone)\n",
      "  Using cached pinecone_client-3.2.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (6.0.1)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.1.52->langchain-pinecone)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3,>=0.1.52->langchain-pinecone)\n",
      "  Using cached langsmith-0.1.82-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (1.10.17)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (8.4.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (2024.6.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (2.2.2)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-pinecone)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain-pinecone)\n",
      "  Downloading orjson-3.10.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m593.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain-pinecone) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain-pinecone) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain-pinecone) (3.7)\n",
      "Using cached langchain_pinecone-0.1.1-py3-none-any.whl (8.4 kB)\n",
      "Using cached langchain_core-0.2.10-py3-none-any.whl (332 kB)\n",
      "Using cached pinecone_client-3.2.2-py3-none-any.whl (215 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langsmith-0.1.82-py3-none-any.whl (127 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading orjson-3.10.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pinecone-client, orjson, jsonpointer, langsmith, jsonpatch, langchain-core, langchain-pinecone\n",
      "  Attempting uninstall: pinecone-client\n",
      "    Found existing installation: pinecone-client 4.1.1\n",
      "    Uninstalling pinecone-client-4.1.1:\n",
      "      Successfully uninstalled pinecone-client-4.1.1\n",
      "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.10 langchain-pinecone-0.1.1 langsmith-0.1.82 orjson-3.10.5 pinecone-client-3.2.2\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain-pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-pinecone in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (0.1.1)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langchain-pinecone) (0.2.10)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langchain-pinecone) (1.26.4)\n",
      "Requirement already satisfied: pinecone-client<4.0.0,>=3.2.2 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langchain-pinecone) (3.2.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (0.1.82)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (1.10.17)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (8.4.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (2024.6.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (2.2.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-pinecone) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain-pinecone) (3.10.5)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain-pinecone) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain-pinecone) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/praveent/micromamba/envs/text/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain-pinecone) (3.7)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade langchain-pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "import os\n",
    "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "docsearch = PineconeVectorStore.from_documents(text_chunks, embeddings,api_key ) #, index_name=index_name)\n",
    "\n",
    "query = \"What are Allergies\"\n",
    "docs = docsearch.similarity_search(query,k=3)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result {'matches': [{'id': '1372',\n",
      "              'metadata': {'text': 'GALE ENCYCLOPEDIA OF MEDICINE 2 '\n",
      "                                   '117Allergies\\n'\n",
      "                                   'Allergic rhinitis is commonly triggered '\n",
      "                                   'by\\n'\n",
      "                                   'exposure to household dust, animal fur,or '\n",
      "                                   'pollen. The foreign substance thattriggers '\n",
      "                                   'an allergic reaction is calledan '\n",
      "                                   'allergen.\\n'\n",
      "                                   'The presence of an allergen causes the\\n'\n",
      "                                   \"body's lymphocytes to begin producingIgE \"\n",
      "                                   'antibodies. The lymphocytes of an allergy '\n",
      "                                   'sufferer produce an unusuallylarge amount '\n",
      "                                   'of IgE.\\n'\n",
      "                                   'IgE molecules attach to mast\\n'\n",
      "                                   'cells, which contain '\n",
      "                                   'histamine.HistaminePollen grains\\n'\n",
      "                                   'Lymphocyte\\n'\n",
      "                                   'FIRST EXPOSURE'},\n",
      "              'score': 0.68253845,\n",
      "              'values': []},\n",
      "             {'id': '1355',\n",
      "              'metadata': {'text': 'allergens are the following:\\n'\n",
      "                                   '• plant pollens\\n'\n",
      "                                   '• animal fur and dander\\n'\n",
      "                                   '• body parts from house mites (microscopic '\n",
      "                                   'creatures\\n'\n",
      "                                   'found in all houses)\\n'\n",
      "                                   '• house dust• mold spores• cigarette '\n",
      "                                   'smoke• solvents• cleaners\\n'\n",
      "                                   'Common food allergens include the '\n",
      "                                   'following:\\n'\n",
      "                                   '• nuts, especially peanuts, walnuts, and '\n",
      "                                   'brazil nuts\\n'\n",
      "                                   '• fish, mollusks, and shellfish• eggs• '\n",
      "                                   'wheat• milk• food additives and '\n",
      "                                   'preservatives\\n'\n",
      "                                   'The following types of drugs commonly '\n",
      "                                   'cause aller-\\n'\n",
      "                                   'gic reactions:\\n'\n",
      "                                   '• penicillin or other antibiotics'},\n",
      "              'score': 0.678439379,\n",
      "              'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'read_units': 6}}\n"
     ]
    }
   ],
   "source": [
    "query = \"What are Allergies\"\n",
    "\n",
    "# create the query vector\n",
    "xq = embeddings.embed_query(query)\n",
    "\n",
    "# now query\n",
    "xc = index.query(vector=xq, top_k=2, include_metadata=True)\n",
    "print(\"Result\", xc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If we already have an index we can load it like this\n",
    "# docsearch=Pinecone.from_existing_index(index_name, embeddings)\n",
    "\n",
    "# query = \"What are Allergies\"\n",
    "\n",
    "# docs=docsearch.similarity_search(query, k=3)\n",
    "\n",
    "# print(\"Result\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/lib64/libm.so.6: version `GLIBC_2.29' not found (required by /home/praveent/micromamba/envs/text/lib/python3.11/site-packages/ctransformers/lib/avx2/libctransformers.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm\u001b[38;5;241m=\u001b[39mCTransformers(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/praveent/End-To-End-Madical-Chatbot-NLP-Project/End-To-End-Madical-Chatbot-NLP-Project/model/llama-2-7b-chat.ggmlv3.q4_0.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                   model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                   config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m      4\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0.8\u001b[39m})\n",
      "File \u001b[0;32m~/micromamba/envs/text/lib/python3.11/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/micromamba/envs/text/lib/python3.11/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/micromamba/envs/text/lib/python3.11/site-packages/pydantic/main.py:1100\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/micromamba/envs/text/lib/python3.11/site-packages/langchain/llms/ctransformers.py:70\u001b[0m, in \u001b[0;36mCTransformers.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import `ctransformers` package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install ctransformers`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m---> 70\u001b[0m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     71\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     72\u001b[0m     model_type\u001b[38;5;241m=\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     73\u001b[0m     model_file\u001b[38;5;241m=\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_file\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     74\u001b[0m     lib\u001b[38;5;241m=\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig,\n\u001b[1;32m     76\u001b[0m )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/micromamba/envs/text/lib/python3.11/site-packages/ctransformers/hub.py:157\u001b[0m, in \u001b[0;36mAutoModelForCausalLM.from_pretrained\u001b[0;34m(cls, model_path_or_repo_id, model_type, model_file, config, lib, local_files_only, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    151\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_find_model_path_from_repo(\n\u001b[1;32m    152\u001b[0m         model_path_or_repo_id,\n\u001b[1;32m    153\u001b[0m         model_file,\n\u001b[1;32m    154\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLM(\n\u001b[1;32m    158\u001b[0m     model_path\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[1;32m    159\u001b[0m     model_type\u001b[38;5;241m=\u001b[39mmodel_type,\n\u001b[1;32m    160\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m    161\u001b[0m     lib\u001b[38;5;241m=\u001b[39mlib,\n\u001b[1;32m    162\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/envs/text/lib/python3.11/site-packages/ctransformers/llm.py:197\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model_path, model_type, config, lib)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(model_path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib \u001b[38;5;241m=\u001b[39m load_library(lib)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib\u001b[38;5;241m.\u001b[39mctransformers_llm_create(\n\u001b[1;32m    199\u001b[0m     model_path\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[1;32m    200\u001b[0m     model_type\u001b[38;5;241m.\u001b[39mencode(),\n\u001b[1;32m    201\u001b[0m     config\u001b[38;5;241m.\u001b[39mcontext_length,\n\u001b[1;32m    202\u001b[0m     config\u001b[38;5;241m.\u001b[39mgpu_layers,\n\u001b[1;32m    203\u001b[0m )\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/text/lib/python3.11/site-packages/ctransformers/llm.py:102\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     99\u001b[0m     os\u001b[38;5;241m.\u001b[39madd_dll_directory(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbin\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    101\u001b[0m path \u001b[38;5;241m=\u001b[39m find_library(path)\n\u001b[0;32m--> 102\u001b[0m lib \u001b[38;5;241m=\u001b[39m CDLL(path)\n\u001b[1;32m    104\u001b[0m lib\u001b[38;5;241m.\u001b[39mctransformers_llm_create\u001b[38;5;241m.\u001b[39margtypes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    105\u001b[0m     c_char_p,  \u001b[38;5;66;03m# model_path\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     c_char_p,  \u001b[38;5;66;03m# model_type\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     c_int,  \u001b[38;5;66;03m# context_length\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     c_int,  \u001b[38;5;66;03m# gpu_layers\u001b[39;00m\n\u001b[1;32m    109\u001b[0m ]\n\u001b[1;32m    110\u001b[0m lib\u001b[38;5;241m.\u001b[39mctransformers_llm_create\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m llm_p\n",
      "File \u001b[0;32m~/micromamba/envs/text/lib/python3.11/ctypes/__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /lib64/libm.so.6: version `GLIBC_2.29' not found (required by /home/praveent/micromamba/envs/text/lib/python3.11/site-packages/ctransformers/lib/avx2/libctransformers.so)"
     ]
    }
   ],
   "source": [
    "llm=CTransformers(model=\"/home/praveent/End-To-End-Madical-Chatbot-NLP-Project/End-To-End-Madical-Chatbot-NLP-Project/model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':512,\n",
    "                          'temperature':0.8})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input=input(f\"Input Prompt:\")\n",
    "    result=qa({\"query\": user_input})\n",
    "    print(\"Response : \", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
